\chapter{Conclusions}

The objective of the project was creation of a system utilizing capture the flag challenges useful for cybersecurity education targetting developers. The problem has been analysed in detail in chapter \ref{chap:problem-analysis}, which helped with the planning and implementation. This objective was successfully achieved and all requirements from chapter \ref{chap:req-and-tools} have been properly validated during testing as described in section \ref{sec:testing-and-verification}. The developed platform allows creating vulnerability descriptions with managed CTF tasks and quizzes. Unlike the alternatives presented in section \ref{sec:existing-solutions} it combines the user interface and challenge management in a single package. Other features unique to this solution are the optional additional challenges related to quiz answers and the per-answer explanations available after solving the quiz. It lacks, however, versatility as it has been developed with web application security in mind. Thanks to the simple challenge management and the system architecture, it should be significantly easier to set up than most alternatives, since the challenge management is automated.
Although the system has not been tested with a broader audience it should be useful for cybersecurity courses at educational institutions and companies from the IT sector. Introduction of gamification such as CTF challenges has been proven to work in these environments. The project is open-source and available on GitHub to everyone who would like to use it. Together with platform exemplary categories and tasks have been prepared to demonstrate the capabilities and present examples for those willing to use it with their own content. Mentioned system content is described in section \ref{sec:categories-of-issues} and partially published on GitHub. For now only the challenge images are \href{https://github.com/krzysdz?ecosystem=container&tab=packages}{public}, because their source code might be considered a \textit{spoiler}, but this decision may be changed at a later time.

The development went without bigger issues, however there were some obstacles. The most prevalent issue was poor documentation of some of the used tools and packages. Notable example is the \href{https://github.com/apocas/dockerode}{dockerode} module, which only shows some examples and links to the Docker Engine API documentation. The behaviour of pulling images is not properly described in neither of those sources and is significantly different from the other actions. Other problems were usually caused by forgetting to pass some options or change a code fragment and are described in section \ref{sec:bugs}.

\section{Future development ideas}

Although the project is usable and demonstrates a general idea there is still an area for improvements. This section lists some propositions for future project development.

\subsection{More content}

The most valuable part of the system for regular users is the content - description of vulnerabilities and tasks. The currently offered examples just scratch the surface of web application security. Expanding the topics, adding more diverse tasks and introducing new categories certainly would enrich the user experience. Broadening the repertoire could make the service useful to a wider audience as well as present the vulnerabilities in more details.

Increasing the number of tasks in a single category with different difficulty levels creates a risk of a less legible and usable category pages and could discourage users less experienced with those categories. To avoid that problem the tasks could be tagged with a difficulty level and ordered by it, so everyone will be able to choose what best matches their abilities and ambitions.

\subsection{Better task management}

Current task management is restricted to task creation, which is rather complicated and involves filling a complex form properly in one try. This aspect can be greatly improved by introducing the following changes:
\begin{itemize}
	\item allow editing existing tasks - ideally the edits would be applied separately to each part (name, description, question, challenge, etc.) of the task to reduce the complexity of changes and make the implementation simpler; this functionality could be used to fix mistakes in the text or update challenges if the image is fixed to a specific version,
	\item task drafts - tasks hidden from users, which may not have all the required fields filled,
	\item health checks and container status - monitoring the status of a challenge container and periodical checks of the application inside could help diagnose issues and prevent users accidentally or purposefully taking the tasks down; could be paired with an alert system to administrators and automated restarts,
	\item forced manual restart - in case a task is down or vandalized and the automatic reset is not going to happen soon, the administrators should be able to force a challenge container reset,
	\item time to the next restart - when the challenge will be recreated from the initial state automatically, this could be also shown on the task page to users,
	\item importing from file - tasks could be imported from a JSON, YAML or other file to help with sharing task configurations between separate deployments,
	\item file attachments - files attached to tasks could be used to share source code to help users and would be useful if someone wanted to use the platform for demonstration of not web-related vulnerabilities.
\end{itemize}

\subsection{User overview for administrators}

To help with debugging possible issues user actions such as flag and answer submission could be logged, even if the flag is invalid. An overview of such events could be then checked by an administrator to verify the user findings, detect misconfigured challenges or hint the user, if for example they found a bait flag left by another user.

\subsection{Progress tracking}

The users can see their progress for a single task when they open it. A progress bar or a colour change could be added to task tiles on category pages to indicate whether a task has been solved and to what level (challenge solved, task answered, all challenges solved).

\subsection{Increasing user engagement}

User engagement could be improved by introducing a public scoreboard and therefore an element of competitiveness. However, publishing their own results should not be mandatory, so as not to discourage the privacy-focused or less confident users.
